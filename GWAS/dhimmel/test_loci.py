import collections
import matplotlib; matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn
from ipdb import set_trace
from os.path import exists,dirname,join,basename 
import pandas as pd 
import numpy as np 
import pickle,json
from tabulate import tabulate
import requests
import os,math,re

from hetio.datasets import datasets_url
from hetio.preproc.downloader import download

test_dir = dirname(__file__)

#
# External dependencies 
#

# windows.tsv is generated by test_windows.py 
dataset_windows = join(test_dir, 'data/snap/', 
    'windows.tsv'
    )

dataset_ebi_df = join(test_dir, 'data/snap/',
    'ebi-df.tsv'
    )

# Read entrez symbol to gene mapping
url_genes_human = join(datasets_url, 'gwas/dhimmel', 
    'entrez-genes-human.tsv'
    )

url_symbols_human = join(datasets_url, 'gwas/dhimmel', 
    'entrez-symbols-human.json'
    )

# Read entrez synonym to genes mapping
url_synonyms_human = join(datasets_url, 'gwas/dhimmel',  
    'entrez-synonyms-human.json'
    )

#
# Check-files and output-files
#

out_snap_associations = join(test_dir, 
    'data/snp-associations.tsv')

out_gene_assocations = join(test_dir, 
    'data/gene-associations.tsv')

#
# Parameters 
#

HC_mlogp = -math.log10(5e-8)    

HC_samples = 1000


def test_loci():
    #
    #   Entrez Genee    
    # 
    localfile = download(
        remote_path=url_symbols_human, 
        download_dir='download'
        )
    symbol_to_id = json.load(open(localfile))

    localfile = download(remote_path=url_synonyms_human, 
        download_dir='download'
        )
    synonym_to_ids = json.load(open(localfile))

    localfile = download(
        remote_path=url_genes_human, download_dir='download'
        )
    
    entrez_df = pd.read_table(localfile)

    # ## Create a set of coding genes
    coding_genes = set(entrez_df.GeneID[
        entrez_df.type_of_gene == 'protein-coding'
            ].astype(str))
  
    # num_coding_genes = len(coding_genes)
    # num_genes = len(entrez_df)

    ## Create a symbol dataframe
    symbol_df = entrez_df[['GeneID', 'Symbol']].rename(
        columns={'GeneID': 'gene', 'Symbol': 'symbol'}
        )
    
    symbol_df.gene = symbol_df.gene.astype(str)

    ebi_df = pd.read_csv(dataset_ebi_df, sep='\t')

    def extract_sample_size(text):
        pattern_cases = r'([\d,]+)[^\d]*?cases'
        pattern_contols = r'([\d,]+)[^\d]*?controls'
        pattern_samples = r'[\d,]+'
        
        remove_commas = lambda x: int(x.replace(',', ''))
        
        sample_size = {'cases': None, 'controls': None, 'samples': None,}
        
        for key, pattern in ('cases', pattern_cases), ('controls', pattern_contols):
            try:
                match = re.search(pattern, text)
                sample_size[key] = remove_commas(match.group(1))
            except (IndexError, AttributeError, ValueError):
                continue
        
        if sample_size['cases'] is not None and sample_size['controls'] is not None:
            sample_size['samples'] = sample_size['cases'] + sample_size['controls']
        
        if sample_size['cases'] is None and sample_size['controls'] is None:
            try:
                match = re.search(pattern_samples, text)
                sample_size['samples'] = remove_commas(match.group(0))
            except (IndexError, AttributeError, ValueError):
                pass
        
        return sample_size

    def process_reported_symbols(text):
        if pd.isnull(text):
            return set()
        
        symbols = set(re.split(r'[, ]+', text))
        symbols -= {'Intergenic', 'NR', 'Pending'}
        entrez_ids = set()
        
        for symbol in symbols:
            gene_id = symbol_to_id.get(symbol)
            if not gene_id:
                gene_ids = synonym_to_ids.get(symbol, [])
                if len(gene_ids) != 1:
                    continue
                gene_id, = gene_ids
            entrez_ids.add(str(gene_id))
        
        return entrez_ids

    # rename and order columns
    rename_dict = {
            'SNP_ID_CURRENT': 'lead_snp', 
            'REPORTED GENE(S)': 'symbols', 
            'PUBMEDID': 'pubmed_id',
            'INITIAL SAMPLE DESCRIPTION': 'sample_description', 
            'PVALUE_MLOG': 'mlog_pvalue',
            'DATE': 'date', 
            'REPORTED GENE(S)': 'reported_symbols', 
            'UPSTREAM_GENE_ID': 'upstream_gene',
            'DOWNSTREAM_GENE_ID': 'downstream_gene', 
            'SNP_GENE_IDS': 'containing_genes'
        }

    columns = ['doid_code', 'doid_name'] + list(rename_dict.values())
    association_df = ebi_df.rename(columns=rename_dict)[columns]

    # split containing genes
    association_df.containing_genes = association_df.containing_genes.map(
        lambda x: set(x.split(';')) if pd.notnull(x) else set()
        )

    # convert reported symbols to entrez GeneIDs
    association_df['reported_genes'] = association_df.reported_symbols.map(
        process_reported_symbols
        )

    # convert dates
    association_df.date = pd.to_datetime(association_df.date)

    # Add rs to SNP names
    association_df.lead_snp = 'rs' + association_df.lead_snp

    # extract sample size information
    sample_size_df = pd.DataFrame(
        list(
            map(
                extract_sample_size, association_df.sample_description)
            )
        )
    
    association_df = association_df.drop(['sample_description'], axis=1)
    association_df = pd.concat([association_df, sample_size_df], axis=1)

    # convert mlog_pavlue to numeric
    association_df.mlog_pvalue = association_df.mlog_pvalue.convert_objects(
        convert_numeric=True
        )
    
    association_df['high_confidence'] = (
        (association_df.mlog_pvalue >= HC_mlogp) &
        (association_df.samples >= HC_samples)).astype(int)

    # 
    # Add window information
    # 

    window_df = pd.read_table(dataset_windows)
    association_df = association_df.merge(
        window_df[['lead_snp', 'lead_chrom', 'lower_coord', 'upper_coord']]
        )

    association_df.head()

    #
    # Identify disease-specific loci
    # 

    def identify_loci(disease_df):
        disease_df = disease_df.sort_values(
            by=['high_confidence', 'mlog_pvalue', 'date'], 
            ascending=False
            )    
        loci = dict()
        for i, row in disease_df.iterrows():
            interval = row.lead_chrom, row.lower_coord, row.upper_coord
            for chrom, lower, upper in loci:
                if interval[0] == chrom and (
                    (lower <= interval[1] and interval[1] <= upper) or
                    (lower <= interval[2] and interval[2] <= upper)):
                    loci[(chrom, lower, upper)].append(row)
                    break
            else:
                loci[interval] = [row]

        # convert each locus to a dataframe
        loci = list(map(pd.DataFrame, loci.values()))

        for i, locus_df in enumerate(loci):
            locus_df['locus'] = i
        
        return pd.concat(loci)

    association_df = association_df.groupby(
        'doid_code', 
        as_index=False).apply(identify_loci)

    # Save associations to tsv
    association_write_df = association_df.copy()
    for column in ['reported_genes', 'containing_genes']:
        association_write_df[column] = association_write_df[column
            ].map(lambda x: '|'.join(x))
    
    association_write_df.to_csv(out_snap_associations, sep='\t', 
        index=False)

    # 
    # Annotate genes to loci
    # 

    def max_counter_keys(counter, key_subset=None):
        if key_subset is not None:
            counter = collections.Counter({k: v for
                k, v in counter.items() if k in key_subset})
        max_value = max(list(counter.values()) + [0])
        max_keys = [k for k, v in counter.items() if v == max_value]
        return max_keys

    def associate_by_gene(locus_df):
        # sort associations by precedence
        locus_df = locus_df.sort_values(by=['high_confidence', 'mlog_pvalue', 'date'], 
            ascending=False)
        
        # Count the number of times each gene is reported across all studies in a loci
        reported_counts = collections.Counter()
        for genes in locus_df.reported_genes:
            reported_counts.update(genes)
        
        primary_gene = None
        
        # Identify a single mode reported gene
        mode_reported = max_counter_keys(reported_counts, coding_genes)
        if len(mode_reported) == 1:
            primary_gene, = mode_reported
        
        # Iterate through associations attempting to resolve primary-gene ambiguity
        for i, row in locus_df.iterrows():
            if primary_gene:
                break

            # Find containing genes
            containing_genes = row.containing_genes & coding_genes
            
            # If a lead SNP is in a gene, consider the containing gene primary
            if len(containing_genes) == 1:
                primary_gene, = containing_genes
                break
            
            # If the lead SNP is in multiple genes, take the mode reported of those genes
            mode_reported = max_counter_keys(reported_counts, containing_genes)
            if len(mode_reported) == 1:
                primary_gene, = mode_reported
                break

            # Take the more reported gene from the upstream and downstream gene
            stream_genes = { gene 
                for gene in [row.downstream_gene, row.upstream_gene] \
                    if pd.notnull(gene)
                    }
            
            mode_reported = max_counter_keys(reported_counts, 
                coding_genes & stream_genes
                )
            
            if len(mode_reported) == 1:
                primary_gene, = mode_reported
                break

        # All genes except the primary are considered secondary
        secondary_genes = set()
        for column in ['reported_genes', 'containing_genes']:
            for genes in locus_df[column]:
                secondary_genes |= genes
        
        secondary_genes |= set(locus_df.upstream_gene)
        secondary_genes |= set(locus_df.downstream_gene)
        secondary_genes = set(filter(pd.notnull, secondary_genes))
        secondary_genes.discard(primary_gene)
        secondary_genes &= coding_genes
        
        # Create a dataframe to return
        columns = ['doid_code', 'doid_name', 'locus', 'high_confidence', 
            'primary', 'gene']

        rows = list()
        
        first_row = locus_df.iloc[0]

        base_row = first_row.doid_code, \
            first_row.doid_name, \
            first_row.locus, \
            first_row.high_confidence

        if primary_gene:
            rows.append(base_row + (1, primary_gene))

        for secondary_gene in secondary_genes:
            rows.append(base_row + (0, secondary_gene))
        
        gene_df = pd.DataFrame(rows, columns=columns)
        
        return gene_df


    grouped = association_df.groupby(['doid_code', 'locus'], 
        as_index=False
        )
    
    gene_df = grouped.apply(associate_by_gene)
    
    for column in 'locus', 'high_confidence', 'primary':
        gene_df[column] = gene_df[column].astype(int)
    
    status = gene_df.apply(
        lambda x: '{}C-{}'.format(
            'H' if x.high_confidence else 'L', 'P' if x.primary else 'S'
            ), 
        axis=1
        )
    
    gene_df.insert(5, 'status', status)
    
    gene_df = gene_df.merge(symbol_df, how='left')

    # Remove duplicated associations 
    gene_df = gene_df.sort_values(
        by=['doid_code', 'high_confidence', 'primary'], 
        ascending=False
        )

    associations_with_dups = len(gene_df)
    
    gene_df = gene_df.drop_duplicates(['doid_code', 'gene'])
    
    print('{} associations removed that were duplicated across statuses'.format(
        associations_with_dups - len(gene_df)) )

    # save gene_df as a tsv
    gene_df.to_csv(out_gene_assocations, 
        sep='\t', 
        index=False
        )
    
    # print(gene_df.head())

    # set_trace()

